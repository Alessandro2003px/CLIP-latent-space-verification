from diffusers import AutoencoderKL
from PIL import Image
import torch, torchvision.transforms as T

device = "cuda" if torch.cuda.is_available() else "cpu"

# VAE di Stable Diffusion 1.5
vae = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae").to(device).eval()

# carica e normalizza l'immagine a 512x512 in [-1, 1]
img = Image.open("mia_immagine.jpg").convert("RGB").resize((512,512))
tfm = T.Compose([T.ToTensor(), T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])
x = tfm(img).unsqueeze(0).to(device)  # [1,3,512,512]

with torch.no_grad():
    latents = vae.encode(x).latent_dist.sample() * 0.18215  # -> [1,4,64,64]
print(latents.shape)
